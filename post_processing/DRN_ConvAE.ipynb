{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "import numpy\n",
    "import os\n",
    "import math\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions, partly taken/adapted from  https://github.com/slerch/ppnn\n",
    "\n",
    "# CRPS loss from Rasp and Lerch (2018)\n",
    "def crps_cost_function(y_true, y_pred):\n",
    "    # Split input\n",
    "    mu = y_pred[:, 0]\n",
    "    sigma = y_pred[:, 1]\n",
    "    y_true = y_true[:, 0]\n",
    "\n",
    "    # To stop sigma from becoming negative we first have to \n",
    "    # convert it the the variance and then take the square\n",
    "    # root again. \n",
    "    var = K.square(sigma)\n",
    "    # The following three variables are just for convenience\n",
    "    loc = (y_true - mu) / K.sqrt(var)\n",
    "    phi = 1.0 / numpy.sqrt(2.0 * numpy.pi) * K.exp(-K.square(loc) / 2.0)\n",
    "    Phi = 0.5 * (1.0 + tf.math.erf(loc / numpy.sqrt(2.0)))\n",
    "    # First we will compute the crps for each input/target pair\n",
    "    crps =  K.sqrt(var) * (loc * (2. * Phi - 1.) + 2 * phi - 1. / numpy.sqrt(numpy.pi))\n",
    "    # Then we take the mean. The cost is now a scalar\n",
    "    return K.mean(crps)\n",
    "\n",
    "def normalize(data, method=None, shift=None, scale=None):\n",
    "    result = numpy.zeros(data.shape)\n",
    "    if method == \"MINMAX\":\n",
    "        shift = numpy.min(data, axis=0)\n",
    "        scale = numpy.max(data, axis=0) - numpy.min(data, axis=0) \n",
    "    elif method == \"STD\":\n",
    "        shift = numpy.mean(data, axis=0)\n",
    "        scale = numpy.std(data, axis=0)\n",
    "    elif method == \"MAX\":\n",
    "        scale = numpy.max(data, axis=0)\n",
    "        shift = numpy.zeros(scale.shape)\n",
    "    for index in range(len(data[0])):\n",
    "        result[:,index] = (data[:,index] - shift[index]) / scale[index]\n",
    "    return result, shift, scale\n",
    "\n",
    "from scipy.stats import norm\n",
    "\n",
    "# Note: needs to be adjusted if GaussianCRPS is used with exp-transformation\n",
    "def crps_normal(mu, sigma, y):\n",
    "    \"\"\"\n",
    "    Compute CRPS for a Gaussian distribution. \n",
    "    \"\"\"\n",
    "    # Make sure sigma is positive\n",
    "    sigma = numpy.abs(sigma)\n",
    "    loc = (y - mu) / sigma\n",
    "    crps = sigma * (loc * (2 * norm.cdf(loc) - 1) + \n",
    "                    2 * norm.pdf(loc) - 1. / numpy.sqrt(numpy.pi))\n",
    "    return crps\n",
    "\n",
    "def save_ensemble(preds, exp_name, save=True):\n",
    "    preds = numpy.array(preds)\n",
    "    preds[:, :, 1] = numpy.abs(preds[:, :, 1])   # Make sure std is positive \n",
    "    mean_preds = numpy.mean(preds, 0)\n",
    "    ens_score = crps_normal(mean_preds[:, 0], mean_preds[:, 1], testY).mean()\n",
    "    # print(f'Ensemble test score = {ens_score}')\n",
    "    if save:\n",
    "        results_df = create_results_df(testDates[:,0], testIDs[:,0], mean_preds[:, 0], mean_preds[:, 1])\n",
    "        results_df.to_csv(f'{exp_name}.csv')\n",
    "    return(ens_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import non-AE data\n",
    "dataRaw = numpy.load('.../data/ppnn.npy')\n",
    "\n",
    "# remove soil moisture forecasts due to missing values\n",
    "data = dataRaw[:,2:39] \n",
    "\n",
    "stations = numpy.genfromtxt('.../data/station_info.csv', delimiter=',', skip_header=1, usecols=[1,2,3,5,6])\n",
    "stationsMap = {}\n",
    "i = 0\n",
    "for station in stations:\n",
    "    stationsMap[int(station[0])] = numpy.concatenate(([i], station[1:]))\n",
    "    i = i + 1\n",
    "stationColumns = numpy.zeros((data.shape[0],4))\n",
    "stationID = numpy.zeros((data.shape[0],1))\n",
    "for t in range(stationColumns.shape[0]):\n",
    "    stationColumns[t] = stationsMap[data[:,1][t]][1:]\n",
    "    stationID[t] = stationsMap[data[:,1][t]][0]\n",
    "       \n",
    "obs = data[:,2]\n",
    "# remove MJD from data\n",
    "data = data[:,3:]\n",
    "\n",
    "data_MJD = dataRaw[:,2].reshape((-1,1))\n",
    "\n",
    "# output:\n",
    "#    data: NWP input features\n",
    "#    stationColumns: station-specific input features (lon, lat, altitude, orography)\n",
    "#    data_MJD: date in MJD format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of parameters to run simulations over all combinations of spatial inputs and encoding dimensions\n",
    "\n",
    "AE_var_inputs_list = [\"t2m\", \"all\", \"500gh\", \"850u\", \"850v\", \"t2m_gh500\"]\n",
    "AE_enc_dim_list = [2,4,8,12,16]\n",
    "AE_early_stopped_list = [True]\n",
    "hidden_layers_list = [2]\n",
    "nodes_hidden_list = [100]\n",
    "emb_dim_list = [15] \n",
    "epochs_list = [100]\n",
    "early_stopping_list = [True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "def expand_grid(dictionary):\n",
    "   return pd.DataFrame([row for row in product(*dictionary.values())], \n",
    "                       columns=dictionary.keys())\n",
    "\n",
    "dictionary = {'AE_var_inputs': AE_var_inputs_list, \n",
    "              'AE_enc_dim': AE_enc_dim_list,\n",
    "              'hidden_layers': hidden_layers_list,\n",
    "              'nodes_hidden': nodes_hidden_list,\n",
    "              'emb_dim': emb_dim_list,\n",
    "              'epochs': epochs_list,\n",
    "              'early_stopping': early_stopping_list}\n",
    "\n",
    "par_list = expand_grid(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sim = par_list.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load raw data\n",
    "dataRaw = numpy.load('../data/ppnn.npy')\n",
    "\n",
    "# remove soil moisture forecasts due to missing values\n",
    "data = dataRaw[:,2:39] \n",
    "\n",
    "stations = numpy.genfromtxt('/home/sebastian/Projects/AE_postprocessing/local_tests/data/station_info.csv', delimiter=',', skip_header=1, usecols=[1,2,3,5,6])\n",
    "stationsMap = {}\n",
    "i = 0\n",
    "for station in stations:\n",
    "    stationsMap[int(station[0])] = numpy.concatenate(([i], station[1:]))\n",
    "    i = i + 1\n",
    "stationColumns = numpy.zeros((data.shape[0],4))\n",
    "stationID = numpy.zeros((data.shape[0],1))\n",
    "for t in range(stationColumns.shape[0]):\n",
    "    stationColumns[t] = stationsMap[data[:,1][t]][1:]\n",
    "    stationID[t] = stationsMap[data[:,1][t]][0]\n",
    "       \n",
    "obs = data[:,2]\n",
    "# remove MJD from data\n",
    "data = data[:,3:]\n",
    "\n",
    "data_MJD = dataRaw[:,2].reshape((-1,1))\n",
    "\n",
    "# output:\n",
    "#    data: NWP input features\n",
    "#    stationColumns: station-specific input features (lon, lat, altitude, orography)\n",
    "#    data_MJD: date in MJD format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for loop to train all considered models\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "for this_simulation in tqdm(range(n_sim)):\n",
    "    \n",
    "    # simulation parameters for this iteration\n",
    "    this_AE_var_input = par_list['AE_var_inputs'][this_simulation]\n",
    "    this_AE_variant = \"simple\"\n",
    "    this_AE_enc_dim = par_list['AE_enc_dim'][this_simulation]\n",
    "    this_AE_early_stopped = \"True\"\n",
    "    this_hidden_layers = par_list['hidden_layers'][this_simulation]\n",
    "    this_nodes_hidden = par_list['nodes_hidden'][this_simulation]\n",
    "    this_emb_dim = par_list['emb_dim'][this_simulation]\n",
    "    this_epochs = par_list['epochs'][this_simulation]\n",
    "    this_early_stopping = par_list['early_stopping'][this_simulation]\n",
    "    \n",
    "    # load relevant AE data\n",
    "    AE_base_dir = '.../AE_results/predictions/ConvAE_'\n",
    "    \n",
    "    fname_t2m = (AE_base_dir + 't2m_' + str(this_AE_enc_dim) + '.npy')\n",
    "    fname_500gh = (AE_base_dir + '500gh_' + str(this_AE_enc_dim) + '.npy')\n",
    "    fname_850u = (AE_base_dir + '850u_' + str(this_AE_enc_dim) + '.npy')\n",
    "    fname_850v = (AE_base_dir + '850v_' + str(this_AE_enc_dim) + '.npy')\n",
    "    \n",
    "    data_AE_t2m_raw = numpy.load(fname_t2m)\n",
    "    data_AE_500gh_raw = numpy.load(fname_500gh)  \n",
    "    data_AE_850u_raw = numpy.load(fname_850u)\n",
    "    data_AE_850v_raw = numpy.load(fname_850v)\n",
    "    \n",
    "    data_AE_t2m_repeated = np.repeat(data_AE_t2m_raw, repeats=537, axis=0)\n",
    "    data_AE_500gh_repeated = np.repeat(data_AE_500gh_raw, repeats=537, axis=0)\n",
    "    data_AE_850u_repeated = np.repeat(data_AE_850u_raw, repeats=537, axis=0)\n",
    "    data_AE_850v_repeated = np.repeat(data_AE_850v_raw, repeats=537, axis=0)\n",
    "    \n",
    "    # remove missing values\n",
    "    eval_start = 1764045\n",
    "    train_end = 1763507 # train until 2015-12-30\n",
    "\n",
    "    trainX_raw = data[:train_end,:]\n",
    "    trainStationData = stationColumns[:train_end,:]\n",
    "    trainY = obs[:train_end]\n",
    "    trainDates = data_MJD[:train_end]\n",
    "    trainIDs = stationID[:train_end]\n",
    "    trainAEpreds_t2m = data_AE_t2m_repeated[:train_end,]\n",
    "    trainAEpreds_500gh = data_AE_500gh_repeated[:train_end,]\n",
    "    trainAEpreds_850u = data_AE_850u_repeated[:train_end,]\n",
    "    trainAEpreds_850v = data_AE_850v_repeated[:train_end,]\n",
    "    \n",
    "    isnans = numpy.isnan(trainY)\n",
    "    trainY = trainY[~isnans]\n",
    "    trainX_raw = trainX_raw[~isnans]\n",
    "    trainStationData = trainStationData[~isnans]\n",
    "    trainDates = trainDates[~isnans]\n",
    "    trainIDs = trainIDs[~isnans]\n",
    "    trainAEpreds_t2m = trainAEpreds_t2m[~isnans]\n",
    "    trainAEpreds_500gh = trainAEpreds_500gh[~isnans]\n",
    "    trainAEpreds_850u = trainAEpreds_850u[~isnans]\n",
    "    trainAEpreds_850v = trainAEpreds_850v[~isnans]\n",
    "\n",
    "    testX_raw = data[eval_start:,:]\n",
    "    testStationData = stationColumns[eval_start:,:]\n",
    "    testY = obs[eval_start:]\n",
    "    testDates = data_MJD[eval_start:]\n",
    "    testIDs = stationID[eval_start:]\n",
    "    testAEpreds_t2m = data_AE_t2m_repeated[eval_start:,]\n",
    "    testAEpreds_500gh = data_AE_500gh_repeated[eval_start:,]\n",
    "    testAEpreds_850u = data_AE_850u_repeated[eval_start:,]\n",
    "    testAEpreds_850v = data_AE_850v_repeated[eval_start:,]\n",
    "\n",
    "    isnans = numpy.isnan(testY)\n",
    "    testY = testY[~isnans]\n",
    "    testX_raw = testX_raw[~isnans]\n",
    "    testStationData = testStationData[~isnans]\n",
    "    testDates = testDates[~isnans]\n",
    "    testIDs = testIDs[~isnans]\n",
    "    testAEpreds_t2m = testAEpreds_t2m[~isnans]\n",
    "    testAEpreds_500gh = testAEpreds_500gh[~isnans]\n",
    "    testAEpreds_850u = testAEpreds_850u[~isnans]\n",
    "    testAEpreds_850v = testAEpreds_850v[~isnans]\n",
    "    \n",
    "    # scale input features (except IDs and AE inputs)\n",
    "    trainX, train_shift, train_scale = normalize(trainX_raw[:,:], method=\"MAX\")\n",
    "    trainStationData, train_shift_StationData, train_scale_StationData = normalize(trainStationData[:,:], \n",
    "                                                                                   method=\"MAX\")\n",
    "    \n",
    "    testX = normalize(testX_raw[:,:], shift=train_shift, scale=train_scale)[0]\n",
    "    testStationData = normalize(testStationData[:,:], \n",
    "                                shift=train_shift_StationData, \n",
    "                                scale=train_scale_StationData)[0]\n",
    "    \n",
    "    # combine AE inputs, depending on variant \n",
    "    if this_AE_var_input == \"t2m\":\n",
    "        trainAEpreds_combined = trainAEpreds_t2m\n",
    "        testAEpreds_combined = testAEpreds_t2m\n",
    "    if this_AE_var_input == \"500gh\":\n",
    "        trainAEpreds_combined = trainAEpreds_500gh\n",
    "        testAEpreds_combined = testAEpreds_500gh\n",
    "    if this_AE_var_input == \"850u\":\n",
    "        trainAEpreds_combined = trainAEpreds_850u\n",
    "        testAEpreds_combined = testAEpreds_850u\n",
    "    if this_AE_var_input == \"850v\":\n",
    "        trainAEpreds_combined = trainAEpreds_850v\n",
    "        testAEpreds_combined = testAEpreds_850v\n",
    "    if this_AE_var_input == \"all\":\n",
    "        trainAEpreds_combined = np.concatenate((trainAEpreds_t2m, \n",
    "                                                trainAEpreds_500gh, \n",
    "                                                trainAEpreds_850u, \n",
    "                                                trainAEpreds_850v), \n",
    "                                               axis = 1)\n",
    "        testAEpreds_combined = np.concatenate((testAEpreds_t2m, \n",
    "                                               testAEpreds_500gh, \n",
    "                                               testAEpreds_850u, \n",
    "                                               testAEpreds_850v), axis = 1)\n",
    "    if this_AE_var_input == \"t2m_gh500\":\n",
    "        trainAEpreds_combined = np.concatenate((trainAEpreds_t2m, \n",
    "                                                trainAEpreds_500gh), \n",
    "                                               axis = 1)\n",
    "        testAEpreds_combined = np.concatenate((testAEpreds_t2m, \n",
    "                                               testAEpreds_500gh), axis = 1)\n",
    "\n",
    "    \n",
    "    # train ensemble of NN models in a loop\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    nreps = 10\n",
    "    trn_scores = []\n",
    "    test_scores = []\n",
    "    preds = []\n",
    "    \n",
    "    n_features = trainX.shape[1] # 34\n",
    "    n_StationFeatures = trainStationData.shape[1] # 4\n",
    "    emb_size = this_emb_dim\n",
    "    n_hidden_layers = this_hidden_layers\n",
    "    max_id = int(numpy.max([trainIDs.max(), testIDs.max()]))\n",
    "    AE_enc_dim = trainAEpreds_combined.shape[1]\n",
    "\n",
    "    hidden_nodes = this_nodes_hidden\n",
    "    activation = 'relu' \n",
    "    optimizer='adam'\n",
    "    lr = 0.002\n",
    "    loss=crps_cost_function \n",
    "    n_outputs = 2\n",
    "    this_epochs == 100:\n",
    "    this_patience = 10\n",
    "        \n",
    "    for i in tqdm(range(nreps), leave=False):\n",
    "        \n",
    "        tf.compat.v1.reset_default_graph()\n",
    "        keras.backend.clear_session()\n",
    "\n",
    "        features_in = tf.keras.layers.Input(shape=(n_features,))\n",
    "        stationdata_in = tf.keras.layers.Input(shape=(n_StationFeatures,))\n",
    "        id_in = tf.keras.layers.Input(shape=(1,))\n",
    "        AE_in = tf.keras.layers.Input(shape=(AE_enc_dim,))\n",
    "\n",
    "        emb = tf.keras.layers.Embedding(max_id + 1, emb_size)(id_in)\n",
    "        emb = tf.keras.layers.Flatten()(emb)\n",
    "\n",
    "        x = tf.keras.layers.Concatenate()([features_in, stationdata_in, emb, AE_in])\n",
    "\n",
    "        hidden_layers = np.repeat(hidden_nodes, n_hidden_layers)\n",
    "        for h in hidden_layers:\n",
    "            x = tf.keras.layers.Dense(h, activation=activation, kernel_regularizer=None)(x)\n",
    "        x = tf.keras.layers.Dense(n_outputs, activation='linear', kernel_regularizer=None)(x)\n",
    "\n",
    "        model = tf.keras.models.Model(inputs=[features_in, stationdata_in, id_in, AE_in], outputs=x)\n",
    "        opt = tf.keras.optimizers.Adam(lr=lr)\n",
    "        model.compile(loss=loss, optimizer=opt)\n",
    "        \n",
    "        if this_early_stopping == True:\n",
    "            es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', \n",
    "                                           patience=this_patience, \n",
    "                                           restore_best_weights = True)\n",
    "            model.fit([trainX, trainStationData, trainIDs, trainAEpreds_combined], \n",
    "                  trainY, \n",
    "                  epochs=this_epochs, \n",
    "                  batch_size=4096, \n",
    "                  verbose=0,\n",
    "                  validation_split=1/9, \n",
    "                  callbacks=[es_callback])\n",
    "        if this_early_stopping == False:\n",
    "            model.fit([trainX, trainStationData, trainIDs, trainAEpreds_combined], \n",
    "                  trainY, \n",
    "                  epochs=this_epochs, \n",
    "                  batch_size=4096, \n",
    "                  verbose=0,\n",
    "                  validation_split=1/9)\n",
    "            \n",
    "        elapsed_time = time.time() - start_time\n",
    "            \n",
    "        trn_scores.append(model.evaluate([trainX, trainStationData, trainIDs, trainAEpreds_combined], \n",
    "                                         trainY, 4096, \n",
    "                                         verbose=0))\n",
    "        preds.append(model.predict([testX, testStationData, testIDs, testAEpreds_combined], \n",
    "                                   4096, verbose=0))\n",
    "        # end for loop for NN model training\n",
    "        \n",
    "    # save predictions\n",
    "    fname_save_preds = ('.../PP_results/predictions/ConvAE_' +\n",
    "                       this_AE_var_input + '_' + \n",
    "                       str(this_AE_enc_dim) + '_' +\n",
    "                       str(this_hidden_layers) + '_' +\n",
    "                       str(this_nodes_hidden) + '_' +\n",
    "                       str(this_emb_dim) + '_' +\n",
    "                       str(this_epochs) + '_' +\n",
    "                       str(this_early_stopping)\n",
    "                      )                           \n",
    "    this_test_score = save_ensemble(preds, fname_save_preds, save = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
