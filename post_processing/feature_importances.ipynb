{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "import numpy\n",
    "import os\n",
    "import math\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions, partly taken/adapted from  https://github.com/slerch/ppnn\n",
    "\n",
    "# CRPS loss from Rasp and Lerch (2018)\n",
    "def crps_cost_function(y_true, y_pred):\n",
    "    # Split input\n",
    "    mu = y_pred[:, 0]\n",
    "    sigma = y_pred[:, 1]\n",
    "    y_true = y_true[:, 0]\n",
    "\n",
    "    # To stop sigma from becoming negative we first have to \n",
    "    # convert it the the variance and then take the square\n",
    "    # root again. \n",
    "    var = K.square(sigma)\n",
    "    # The following three variables are just for convenience\n",
    "    loc = (y_true - mu) / K.sqrt(var)\n",
    "    phi = 1.0 / numpy.sqrt(2.0 * numpy.pi) * K.exp(-K.square(loc) / 2.0)\n",
    "    Phi = 0.5 * (1.0 + tf.math.erf(loc / numpy.sqrt(2.0)))\n",
    "    # First we will compute the crps for each input/target pair\n",
    "    crps =  K.sqrt(var) * (loc * (2. * Phi - 1.) + 2 * phi - 1. / numpy.sqrt(numpy.pi))\n",
    "    # Then we take the mean. The cost is now a scalar\n",
    "    return K.mean(crps)\n",
    "\n",
    "def normalize(data, method=None, shift=None, scale=None):\n",
    "    result = numpy.zeros(data.shape)\n",
    "    if method == \"MINMAX\":\n",
    "        shift = numpy.min(data, axis=0)\n",
    "        scale = numpy.max(data, axis=0) - numpy.min(data, axis=0) \n",
    "    elif method == \"STD\":\n",
    "        shift = numpy.mean(data, axis=0)\n",
    "        scale = numpy.std(data, axis=0)\n",
    "    elif method == \"MAX\":\n",
    "        scale = numpy.max(data, axis=0)\n",
    "        shift = numpy.zeros(scale.shape)\n",
    "    for index in range(len(data[0])):\n",
    "        result[:,index] = (data[:,index] - shift[index]) / scale[index]\n",
    "    return result, shift, scale\n",
    "\n",
    "from scipy.stats import norm\n",
    "\n",
    "# Note: needs to be adjusted if GaussianCRPS is used with exp-transformation\n",
    "def crps_normal(mu, sigma, y):\n",
    "    \"\"\"\n",
    "    Compute CRPS for a Gaussian distribution. \n",
    "    \"\"\"\n",
    "    # Make sure sigma is positive\n",
    "    sigma = numpy.abs(sigma)\n",
    "    loc = (y - mu) / sigma\n",
    "    crps = sigma * (loc * (2 * norm.cdf(loc) - 1) + \n",
    "                    2 * norm.pdf(loc) - 1. / numpy.sqrt(numpy.pi))\n",
    "    return crps\n",
    "\n",
    "def save_ensemble(preds, exp_name, save=True):\n",
    "    preds = numpy.array(preds)\n",
    "    preds[:, :, 1] = numpy.abs(preds[:, :, 1])   # Make sure std is positive \n",
    "    mean_preds = numpy.mean(preds, 0)\n",
    "    ens_score = crps_normal(mean_preds[:, 0], mean_preds[:, 1], testY).mean()\n",
    "    # print(f'Ensemble test score = {ens_score}')\n",
    "    if save:\n",
    "        results_df = create_results_df(testDates[:,0], testIDs[:,0], mean_preds[:, 0], mean_preds[:, 1])\n",
    "        results_df.to_csv(f'{exp_name}.csv')\n",
    "    return(ens_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import non-AE data\n",
    "dataRaw = numpy.load('/home/sebastian/Projects/AE_postprocessing/local_tests/data/ppnn.npy')\n",
    "\n",
    "# remove soil moisture forecasts due to missing values\n",
    "data = dataRaw[:,2:39] \n",
    "\n",
    "stations = numpy.genfromtxt('/home/sebastian/Projects/AE_postprocessing/local_tests/data/station_info.csv', delimiter=',', skip_header=1, usecols=[1,2,3,5,6])\n",
    "stationsMap = {}\n",
    "i = 0\n",
    "for station in stations:\n",
    "    stationsMap[int(station[0])] = numpy.concatenate(([i], station[1:]))\n",
    "    i = i + 1\n",
    "stationColumns = numpy.zeros((data.shape[0],4))\n",
    "stationID = numpy.zeros((data.shape[0],1))\n",
    "for t in range(stationColumns.shape[0]):\n",
    "    stationColumns[t] = stationsMap[data[:,1][t]][1:]\n",
    "    stationID[t] = stationsMap[data[:,1][t]][0]\n",
    "       \n",
    "obs = data[:,2]\n",
    "# remove MJD from data\n",
    "data = data[:,3:]\n",
    "\n",
    "data_MJD = dataRaw[:,2].reshape((-1,1))\n",
    "\n",
    "# output:\n",
    "#    data: NWP input features\n",
    "#    stationColumns: station-specific input features (lon, lat, altitude, orography)\n",
    "#    data_MJD: date in MJD format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_data = np.array(['t2m_fc_mean', 't2m_fc_std','u_pl500_fc_mean', 'u_pl500_fc_std',\n",
    "       'v_pl500_fc_mean', 'v_pl500_fc_std', 'gh_pl500_fc_mean',\n",
    "       'gh_pl500_fc_std', 'u_pl850_fc_mean', 'u_pl850_fc_std',\n",
    "       'v_pl850_fc_mean', 'v_pl850_fc_std', 'q_pl850_fc_mean',\n",
    "       'q_pl850_fc_std', 'cape_fc_mean', 'cape_fc_std', 'sp_fc_mean',\n",
    "       'sp_fc_std', 'tcc_fc_mean', 'tcc_fc_std', 'sshf_fc_mean',\n",
    "       'sshf_fc_std', 'slhf_fc_mean', 'slhf_fc_std', 'u10_fc_mean',\n",
    "       'u10_fc_std', 'v10_fc_mean', 'v10_fc_std', 'ssr_fc_mean',\n",
    "       'ssr_fc_std', 'str_fc_mean', 'str_fc_std', 'd2m_fc_mean',\n",
    "       'd2m_fc_std'])\n",
    "\n",
    "names_stationColumns = np.array(['station_lat',\n",
    "       'station_lon', 'orog', 'station_alt', ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "this_AE_var_input = \"t2m\"\n",
    "this_AE_variant = \"simple\"\n",
    "this_AE_enc_dim = 2\n",
    "this_AE_early_stopped = \"True\"\n",
    "this_hidden_layers = 2\n",
    "this_nodes_hidden = 100\n",
    "this_emb_dim = 15\n",
    "this_epochs = 100\n",
    "this_early_stopping = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load relevant AE data\n",
    "\n",
    "AE_base_dir = '/home/sebastian/Projects/AE_postprocessing/results_paper/AE_results/predictions/ConvAE_'\n",
    "\n",
    "fname_t2m = (AE_base_dir + 't2m_' + str(this_AE_enc_dim) + '.npy')\n",
    "\n",
    "data_AE_t2m_raw = numpy.load(fname_t2m)\n",
    "data_AE_t2m_repeated = np.repeat(data_AE_t2m_raw, repeats=537, axis=0)\n",
    "\n",
    "# remove missing values\n",
    "eval_start = 1764045\n",
    "train_end = 1763507 # train until 2015-12-30\n",
    "\n",
    "trainX_raw = data[:train_end,:]\n",
    "trainStationData = stationColumns[:train_end,:]\n",
    "trainY = obs[:train_end]\n",
    "trainDates = data_MJD[:train_end]\n",
    "trainIDs = stationID[:train_end]\n",
    "trainAEpreds_t2m = data_AE_t2m_repeated[:train_end,]\n",
    "\n",
    "isnans = numpy.isnan(trainY)\n",
    "trainY = trainY[~isnans]\n",
    "trainX_raw = trainX_raw[~isnans]\n",
    "trainStationData = trainStationData[~isnans]\n",
    "trainDates = trainDates[~isnans]\n",
    "trainIDs = trainIDs[~isnans]\n",
    "trainAEpreds_t2m = trainAEpreds_t2m[~isnans]\n",
    "\n",
    "testX_raw = data[eval_start:,:]\n",
    "testStationData = stationColumns[eval_start:,:]\n",
    "testY = obs[eval_start:]\n",
    "testDates = data_MJD[eval_start:]\n",
    "testIDs = stationID[eval_start:]\n",
    "testAEpreds_t2m = data_AE_t2m_repeated[eval_start:,]\n",
    "\n",
    "isnans = numpy.isnan(testY)\n",
    "testY = testY[~isnans]\n",
    "testX_raw = testX_raw[~isnans]\n",
    "testStationData = testStationData[~isnans]\n",
    "testDates = testDates[~isnans]\n",
    "testIDs = testIDs[~isnans]\n",
    "testAEpreds_t2m = testAEpreds_t2m[~isnans]\n",
    "\n",
    "# scale input features (except IDs and AE inputs)\n",
    "trainX, train_shift, train_scale = normalize(trainX_raw[:,:], method=\"MAX\")\n",
    "trainStationData, train_shift_StationData, train_scale_StationData = normalize(trainStationData[:,:], \n",
    "                                                                               method=\"MAX\")\n",
    "\n",
    "testX = normalize(testX_raw[:,:], shift=train_shift, scale=train_scale)[0]\n",
    "testStationData = normalize(testStationData[:,:], \n",
    "                            shift=train_shift_StationData, \n",
    "                            scale=train_scale_StationData)[0]\n",
    "\n",
    "# combine AE inputs\n",
    "if this_AE_var_input == \"t2m\":\n",
    "    trainAEpreds_combined = trainAEpreds_t2m\n",
    "    testAEpreds_combined = testAEpreds_t2m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_AE = np.array(['AE'])\n",
    "\n",
    "nreps = 10\n",
    "fimps = []\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "for i in tqdm(range(nreps)):\n",
    "    \n",
    "    n_features = trainX.shape[1] # 34\n",
    "    n_StationFeatures = trainStationData.shape[1] # 4\n",
    "    emb_size = this_emb_dim\n",
    "    n_hidden_layers = this_hidden_layers\n",
    "    max_id = int(numpy.max([trainIDs.max(), testIDs.max()]))\n",
    "    AE_enc_dim = trainAEpreds_combined.shape[1]\n",
    "\n",
    "    hidden_nodes = this_nodes_hidden\n",
    "    activation = 'relu' \n",
    "    optimizer='adam'\n",
    "    lr = 0.002\n",
    "    loss=crps_cost_function \n",
    "    n_outputs = 2\n",
    "    this_patience = 10\n",
    "\n",
    "    tf.compat.v1.reset_default_graph()\n",
    "    keras.backend.clear_session()\n",
    "\n",
    "    features_in = tf.keras.layers.Input(shape=(n_features,))\n",
    "    stationdata_in = tf.keras.layers.Input(shape=(n_StationFeatures,))\n",
    "    id_in = tf.keras.layers.Input(shape=(1,))\n",
    "    AE_in = tf.keras.layers.Input(shape=(AE_enc_dim,))\n",
    "\n",
    "    emb = tf.keras.layers.Embedding(max_id + 1, emb_size)(id_in)\n",
    "    emb = tf.keras.layers.Flatten()(emb)\n",
    "\n",
    "    x = tf.keras.layers.Concatenate()([features_in, stationdata_in, emb, AE_in])\n",
    "\n",
    "    hidden_layers = np.repeat(hidden_nodes, n_hidden_layers)\n",
    "    for h in hidden_layers:\n",
    "        x = tf.keras.layers.Dense(h, activation=activation, kernel_regularizer=None)(x)\n",
    "    x = tf.keras.layers.Dense(n_outputs, activation='linear', kernel_regularizer=None)(x)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=[features_in, stationdata_in, id_in, AE_in], outputs=x)\n",
    "    opt = tf.keras.optimizers.Adam(lr=lr)\n",
    "    model.compile(loss=loss, optimizer=opt)\n",
    "\n",
    "    es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', \n",
    "                                   patience=this_patience, \n",
    "                                   restore_best_weights = True)\n",
    "    model.fit([trainX, trainStationData, trainIDs, trainAEpreds_combined], \n",
    "          trainY, \n",
    "          epochs=this_epochs, \n",
    "          batch_size=4096, \n",
    "          verbose=0,\n",
    "          validation_split=1/9, \n",
    "          callbacks=[es_callback])\n",
    "    \n",
    "    ref_score = model.evaluate([testX, testStationData, testIDs, testAEpreds_combined], \n",
    "                                         testY, 4096, \n",
    "                                         verbose=0)\n",
    "    \n",
    "    # permute inputs for feature importance computations -> by type of input\n",
    "    \n",
    "    ## NWP inputs interpolated to stations\n",
    "    \n",
    "    scores_permuted_NWPinputs = np.zeros(len(names_data))\n",
    "\n",
    "    for j in range(len(names_data)):\n",
    "        testX_shuf = testX.copy()\n",
    "        testX_shuf[:, j] = np.random.permutation(testX_shuf[:, j])\n",
    "        this_score_shuf = model.evaluate([testX_shuf, testStationData, testIDs, testAEpreds_combined], \n",
    "                                             testY, 4096, verbose=0)\n",
    "        scores_permuted_NWPinputs[j] = this_score_shuf\n",
    "     \n",
    "    ## station info\n",
    "    \n",
    "    scores_permuted_stationinfo = np.zeros(len(names_stationColumns))\n",
    "\n",
    "    for j in range(len(names_stationColumns)):\n",
    "        testStationData_shuf = testStationData.copy()\n",
    "        testStationData_shuf[:, j] = np.random.permutation(testStationData_shuf[:, j])\n",
    "        this_score_shuf = model.evaluate([testX, testStationData_shuf, testIDs, testAEpreds_combined], \n",
    "                                             testY, 4096, verbose=0)\n",
    "        scores_permuted_stationinfo[j] = this_score_shuf\n",
    "    \n",
    "    ## station ID\n",
    "    \n",
    "    scores_permuted_stationID = np.zeros(1)\n",
    "\n",
    "    testIDs_shuf = testIDs.copy()\n",
    "    testIDs_shuf[:,0] = np.random.permutation(testIDs_shuf[:,0])\n",
    "    this_score_shuf = model.evaluate([testX, testStationData, testIDs_shuf, testAEpreds_combined], \n",
    "                                          testY, 4096, verbose=0)\n",
    "    scores_permuted_stationID = this_score_shuf\n",
    "    \n",
    "    ## AE inputs\n",
    "    \n",
    "    scores_permuted_AE = np.zeros(len(names_AE))\n",
    "    testAEpreds_combined_shuf = testAEpreds_combined.copy()\n",
    "    shuf_rows = np.random.permutation(range(testAEpreds_combined_shuf.shape[0]))\n",
    "    testAEpreds_combined_shuf = testAEpreds_combined_shuf[shuf_rows, :]\n",
    "    this_score_shuf = model.evaluate([testX, testStationData, testIDs, testAEpreds_combined_shuf], \n",
    "                                         testY, 4096, verbose=0)\n",
    "    scores_permuted_AE = this_score_shuf\n",
    "\n",
    "    scores_permuted_AE = np.atleast_1d(scores_permuted_AE)\n",
    "    \n",
    "    # combine all and compute importances\n",
    "    \n",
    "    scores_shuffled = np.concatenate((scores_permuted_NWPinputs, scores_permuted_stationinfo, np.atleast_1d(scores_permuted_stationID), scores_permuted_AE),\n",
    "                                axis = 0)\n",
    "    \n",
    "    feature_names =  np.concatenate((names_data, \n",
    "                                 names_stationColumns, \n",
    "                                 np.atleast_1d(['station ID']), \n",
    "                                 names_AE),\n",
    "                                axis = 0)\n",
    "    \n",
    "    fimps.insert(0, scores_shuffled - ref_score)\n",
    "    \n",
    "# compute mean feature importance and convert to data frame\n",
    "\n",
    "fimps = np.array(fimps).transpose()\n",
    "mean_fimps = fimps.mean(axis = 1)\n",
    "df = pd.DataFrame(columns=['Feature', 'Mean_Importance'])\n",
    "df['Feature'] = feature_names; df['Mean_Importance'] = mean_fimps\n",
    "df.sort_values('Mean_Importance')\n",
    "\n",
    "fimps_df.to_csv('.../PP_results/feature_importances_ConvAE.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
